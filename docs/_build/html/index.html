<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to bitsandbytes’s documentation! &mdash; bitsandbytes v0.0.24 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> bitsandbytes
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to bitsandbytes’s documentation!</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">bitsandbytes</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Welcome to bitsandbytes’s documentation!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="module-bitsandbytes.functional">
<span id="welcome-to-bitsandbytes-s-documentation"></span><h1>Welcome to bitsandbytes’s documentation!<a class="headerlink" href="#module-bitsandbytes.functional" title="Permalink to this headline">¶</a></h1>
<dl class="py function">
<dt id="bitsandbytes.functional.create_dynamic_map">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">create_dynamic_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">signed</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">n</span><span class="o">=</span><span class="default_value">7</span></em><span class="sig-paren">)</span><a class="headerlink" href="#bitsandbytes.functional.create_dynamic_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the dynamic quantiztion map.</p>
<p>The dynamic data type is made up of a dynamic exponent and
fraction. As the exponent increase from 0 to -7 the number
of bits available for the fraction shrinks.</p>
<p>This is a generalization of the dynamic type where a certain
number of the bits and be reserved for the linear quantization
region (the fraction). n determines the maximum number of
exponent bits.</p>
<p>For more details see
(8-Bit Approximations for Parallelism in Deep Learning)[<a class="reference external" href="https://arxiv.org/abs/1511.04561">https://arxiv.org/abs/1511.04561</a>]</p>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.dequantize_blockwise">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">dequantize_blockwise</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">quant_state</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">absmax</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">code</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">blocksize</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">4096</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#bitsandbytes.functional.dequantize_blockwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Dequantizes blockwise quantized values.</p>
<p>Dequantizes the tensor A with maximum absolute values absmax in
blocks of size 4096.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<em>torch.Tensor</em>) – The input 8-bit tensor.</p></li>
<li><p><strong>quant_state</strong> (<em>tuple</em><em>(</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>)</em>) – Tuple of code and absmax values.</p></li>
<li><p><strong>absmax</strong> (<em>torch.Tensor</em>) – The absmax values.</p></li>
<li><p><strong>code</strong> (<em>torch.Tensor</em>) – The quantization map.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – Dequantized output tensor (default: float32)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dequantized tensor (default: float32)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.dequantize_no_absmax">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">dequantize_no_absmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">code</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#bitsandbytes.functional.dequantize_no_absmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Dequantizes the 8-bit tensor to 32-bit.</p>
<p>Dequantizes the 8-bit tensor <cite>A</cite> to the 32-bit tensor <cite>out</cite> via
the quantization map <cite>code</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<em>torch.Tensor</em>) – The 8-bit input tensor.</p></li>
<li><p><strong>code</strong> (<em>torch.Tensor</em>) – The quantization map.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – The 32-bit output tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>32-bit output tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.estimate_quantiles">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">estimate_quantiles</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">offset</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.001953125</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#bitsandbytes.functional.estimate_quantiles" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates 256 equidistant quantiles on the input tensor eCDF.</p>
<p>Uses SRAM-Quantiles algorithm to quickly estimate 256 equidistant quantiles
via the eCDF of the input tensor <cite>A</cite>. This is a fast but approximate algorithm
and the extreme quantiles close to 0 and 1 have high variance / large estimation
errors. These large errors can be avoided by using the offset variable which trims
the distribution. The default offset value of 1/512 ensures minimum entropy encoding – it
trims 1/512 = 0.2% from each side of the distrivution. An offset value of 0.01 to 0.02
usually has a much lower error but is not a minimum entropy encoding. Given an offset
of 0.02 equidistance points in the range [0.02, 0.98] are used for the quantiles.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<em>torch.Tensor</em>) – The input tensor. Any shape.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – Tensor with the 256 estimated quantiles.</p></li>
<li><p><strong>offset</strong> (<em>float</em>) – The offset for the first and last quantile from 0 and 1. Default: 1/512</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The 256 quantiles in float32 datatype.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.get_ptr">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">get_ptr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; ctypes.c_void_p<a class="headerlink" href="#bitsandbytes.functional.get_ptr" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the ctypes pointer from a PyTorch Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>A</strong> (<em>torch.tensor</em>) – The PyTorch tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ctypes.c_void_p</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.optimizer_update_32bit">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">optimizer_update_32bit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">g</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">p</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">state1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">state2</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">beta2</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">weight_decay</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">gnorm_scale</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">unorm_vec</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_unorm</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#bitsandbytes.functional.optimizer_update_32bit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace optimizer update with one or two optimizer states.</p>
<p>Universal optimizer update for 32-bit state and 32/16-bit gradients/weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer_name</strong> (<em>str</em>) – The name of the optimizer: {adam}.</p></li>
<li><p><strong>g</strong> (<em>torch.Tensor</em>) – Gradient tensor.</p></li>
<li><p><strong>p</strong> (<em>torch.Tensor</em>) – Parameter tensor.</p></li>
<li><p><strong>state1</strong> (<em>torch.Tensor</em>) – Optimizer state 1.</p></li>
<li><p><strong>beta1</strong> (<em>float</em>) – Optimizer beta1.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – Optimizer epsilon.</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em>) – Weight decay.</p></li>
<li><p><strong>step</strong> (<em>int</em>) – Current optimizer step.</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – The learning rate.</p></li>
<li><p><strong>state2</strong> (<em>torch.Tensor</em>) – Optimizer state 2.</p></li>
<li><p><strong>beta2</strong> (<em>float</em>) – Optimizer beta2.</p></li>
<li><p><strong>gnorm_scale</strong> (<em>float</em>) – The factor to rescale the gradient to the max clip value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.optimizer_update_8bit">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">optimizer_update_8bit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">g</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">p</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">state1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">state2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">beta2</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">qmap1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">qmap2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">max1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">max2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">new_max1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">new_max2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">weight_decay</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">gnorm_scale</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">unorm_vec</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_unorm</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#bitsandbytes.functional.optimizer_update_8bit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace Adam update.</p>
<p>Universal Adam update for 32/8-bit state and 32/16-bit gradients/weights.
Uses AdamW formulation if weight decay &gt; 0.0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer_name</strong> (<em>str</em>) – The name of the optimizer. Choices {adam, momentum}</p></li>
<li><p><strong>g</strong> (<em>torch.Tensor</em>) – Gradient tensor.</p></li>
<li><p><strong>p</strong> (<em>torch.Tensor</em>) – Parameter tensor.</p></li>
<li><p><strong>state1</strong> (<em>torch.Tensor</em>) – Adam state 1.</p></li>
<li><p><strong>state2</strong> (<em>torch.Tensor</em>) – Adam state 2.</p></li>
<li><p><strong>beta1</strong> (<em>float</em>) – Adam beta1.</p></li>
<li><p><strong>beta2</strong> (<em>float</em>) – Adam beta2.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – Adam epsilon.</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em>) – Weight decay.</p></li>
<li><p><strong>step</strong> (<em>int</em>) – Current optimizer step.</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – The learning rate.</p></li>
<li><p><strong>qmap1</strong> (<em>torch.Tensor</em>) – Quantization map for first Adam state.</p></li>
<li><p><strong>qmap2</strong> (<em>torch.Tensor</em>) – Quantization map for second Adam state.</p></li>
<li><p><strong>max1</strong> (<em>torch.Tensor</em>) – Max value for first Adam state update.</p></li>
<li><p><strong>max2</strong> (<em>torch.Tensor</em>) – Max value for second Adam state update.</p></li>
<li><p><strong>new_max1</strong> (<em>torch.Tensor</em>) – Max value for the next Adam update of the first state.</p></li>
<li><p><strong>new_max2</strong> (<em>torch.Tensor</em>) – Max value for the next Adam update of the second state.</p></li>
<li><p><strong>gnorm_scale</strong> (<em>float</em>) – The factor to rescale the gradient to the max clip value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.percentile_clipping">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">percentile_clipping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">grad</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">gnorm_vec</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">percentile</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">5</span></em><span class="sig-paren">)</span><a class="headerlink" href="#bitsandbytes.functional.percentile_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies percentile clipping</p>
<dl class="simple">
<dt>grad: torch.Tensor</dt><dd><p>The gradient tensor.</p>
</dd>
<dt>gnorm_vec: torch.Tensor</dt><dd><p>Vector of gradient norms. 100 elements expected.</p>
</dd>
<dt>step: int</dt><dd><p>The current optimiation steps (number of past gradient norms).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.quantize_blockwise">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">quantize_blockwise</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">code</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">absmax</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">rand</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#bitsandbytes.functional.quantize_blockwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize tensor A in blocks of size 4096 values.</p>
<p>Quantizes tensor A by dividing it into blocks of 4096 values.
Then the absolute maximum value within these blocks is calculated
for the non-linear quantization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>code</strong> (<em>torch.Tensor</em>) – The quantization map.</p></li>
<li><p><strong>absmax</strong> (<em>torch.Tensor</em>) – The absmax values.</p></li>
<li><p><strong>rand</strong> (<em>torch.Tensor</em>) – The tensor for stochastic rounding.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – The output tensor (8-bit).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>torch.Tensor</em> – The 8-bit tensor.</p></li>
<li><p><em>tuple(torch.Tensor, torch.Tensor)</em> – The quantization state to undo the quantization.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="bitsandbytes.functional.quantize_no_absmax">
<code class="sig-prename descclassname">bitsandbytes.functional.</code><code class="sig-name descname">quantize_no_absmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">A</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">code</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">torch.Tensor</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#bitsandbytes.functional.quantize_no_absmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantizes input tensor to 8-bit.</p>
<p>Quantizes the 32-bit input tensor <cite>A</cite> to the 8-bit output tensor
<cite>out</cite> using the quantization map <cite>code</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>code</strong> (<em>torch.Tensor</em>) – The quantization map.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The output tensor. Needs to be of type byte.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Quantized 8-bit tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Facebook, Inc. and its affiliates..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>